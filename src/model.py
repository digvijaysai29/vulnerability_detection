# src/model.py

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.initializers import Constant
import numpy as np


def load_glove_embeddings(tokenizer, embedding_dim=100):
    """
    Load pre-trained GloVe embeddings.

    Parameters:
    - tokenizer: Keras Tokenizer object used for tokenizing text.
    - embedding_dim: Dimension of GloVe embeddings (default is 100).

    Returns:
    - embedding_matrix: Matrix of GloVe embeddings for words in the tokenizer's vocabulary.
    """

    # Load GloVe embeddings from file (make sure you have downloaded glove.6B.100d.txt)
    embedding_index = {}

    with open('glove.6B.100d.txt', 'r') as f:
        for line in f:
            values = line.split()
            word = values[0]
            coefs = np.asarray(values[1:], dtype='float32')
            embedding_index[word] = coefs

    # Prepare embedding matrix
    vocab_size = len(tokenizer.word_index) + 1  # Add 1 for padding token
    embedding_matrix = np.zeros((vocab_size, embedding_dim))

    for word, i in tokenizer.word_index.items():
        embedding_vector = embedding_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector

    return embedding_matrix


def build_model(input_dim, output_dim, embedding_matrix=None):
    """
    Build an LSTM model for vulnerability detection with optional pre-trained embeddings.

    Parameters:
    - input_dim: Size of the input vocabulary (e.g., number of unique tokens or features).
    - output_dim: Number of output classes (e.g., number of unique CWEs).
    - embedding_matrix: Pre-trained word embeddings matrix (optional).

    Returns:
    - model: Compiled LSTM model.
    """
    model = Sequential()

    if embedding_matrix is not None:
        # Use pre-trained GloVe embeddings
        embedding_layer = Embedding(input_dim=input_dim,
                                    output_dim=embedding_matrix.shape[1],
                                    embeddings_initializer=Constant(embedding_matrix),
                                    trainable=False)  # Keep embeddings fixed during training
        model.add(embedding_layer)
    else:
        # Randomly initialized embeddings if no pre-trained embeddings are provided
        model.add(Embedding(input_dim=input_dim, output_dim=128))

    # LSTM layer with dropout for regularization
    model.add(LSTM(128))
    model.add(Dropout(0.5))  # Dropout to prevent overfitting

    # Dense layer with softmax activation for multi-class classification
    model.add(Dense(output_dim, activation='softmax'))

    # Compile the model with Adam optimizer and categorical crossentropy loss
    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

    return model


if __name__ == "__main__":
    input_dim = 10000  # Example input dimension (vocabulary size from tokenizer)
    output_dim = 50  # Example output dimension (number of classes)

    # Load tokenizer here if needed to pass into load_glove_embeddings()

    # Example usage with pre-trained GloVe embeddings
    embedding_matrix = load_glove_embeddings(tokenizer=None)  # Pass actual tokenizer here

    model = build_model(input_dim=input_dim, output_dim=output_dim, embedding_matrix=embedding_matrix)

    print(model.summary())